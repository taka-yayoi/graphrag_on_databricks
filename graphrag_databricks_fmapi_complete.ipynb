{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c1b963-a7fb-4e66-b99a-1bc8c2d4cd9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Microsoft GraphRAG on Databricks (FMAPI版)\n",
    "\n",
    "このノートブックでは、MicrosoftのGraphRAGをDatabricks上で実行する方法を説明します。\n",
    "\n",
    "## GraphRAGとは\n",
    "\n",
    "GraphRAGは、ナレッジグラフを活用したRAG(Retrieval-Augmented Generation)手法です。\n",
    "従来のベクトル検索ベースのRAGと異なり、以下の特徴があります:\n",
    "\n",
    "- **エンティティとリレーションシップの抽出**: テキストから人物、組織、技術などのエンティティと、それらの関係性を抽出\n",
    "- **コミュニティ検出**: 関連するエンティティをクラスタリングし、階層的なコミュニティ構造を構築\n",
    "- **Global Search**: コミュニティレポートを活用し、データセット全体に関する高レベルな質問に回答\n",
    "- **Local Search**: 特定のエンティティに関する詳細な質問に回答\n",
    "\n",
    "## 前提条件\n",
    "\n",
    "- Databricks Runtime 14.0 ML以上\n",
    "- **クラスター**(サーバレスではなく)を使用(LanceDBのファイル操作制約のため)\n",
    "- FMAPIでOpenAIモデル(databricks-gpt-5-2等)が利用可能であること\n",
    "- Unity Catalogが有効なワークスペース\n",
    "\n",
    "## 参考リンク\n",
    "\n",
    "- [GraphRAG GitHub](https://github.com/microsoft/graphrag)\n",
    "- [GraphRAG Documentation](https://microsoft.github.io/graphrag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6ab4db-7eea-4415-85fa-97ae4385ad5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. パッケージのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d5d547-bb29-46b5-9ba6-1df83023d643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-engineering 0.12.1 requires azure-cosmos==4.3.1, but you have azure-cosmos 4.14.3 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install graphrag pyvis --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131231d7-cc32-45a8-88c7-c70a5cbddf23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166d0623-cc2e-49a7-9580-a81714563503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. 作業ディレクトリの設定\n",
    "\n",
    "GraphRAGはLanceDBを使用してベクトルデータを保存しますが、LanceDBの`rename`操作は\n",
    "Unity CatalogボリュームやWorkspaceファイルシステムでサポートされていません。\n",
    "そのため、**ローカルディスク(`/tmp`)** を作業ディレクトリとして使用します。\n",
    "\n",
    "処理完了後、結果はDelta Tableに保存して永続化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e3cbab-2a4f-44d0-a705-ecdbd40748a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作業ディレクトリ: /tmp/graphrag_work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Unity Catalog設定\n",
    "CATALOG = \"takaakiyayoi_catalog\"\n",
    "SCHEMA = \"default\"\n",
    "\n",
    "# 作業ディレクトリ(クラスターのローカルディスク)\n",
    "WORK_DIR = \"/tmp/graphrag_work\"\n",
    "INPUT_DIR = f\"{WORK_DIR}/input\"\n",
    "OUTPUT_DIR = f\"{WORK_DIR}/output\"\n",
    "CACHE_DIR = f\"{WORK_DIR}/cache\"\n",
    "\n",
    "# 既存のディレクトリがあればクリア\n",
    "if os.path.exists(WORK_DIR):\n",
    "    shutil.rmtree(WORK_DIR)\n",
    "\n",
    "# ディレクトリ作成\n",
    "for d in [WORK_DIR, INPUT_DIR, OUTPUT_DIR, CACHE_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"作業ディレクトリ: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c66b26f-7581-4fe8-a8eb-aa19367c6127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. サンプルデータの準備\n",
    "\n",
    "GraphRAGのインデックス作成には、テキストデータが必要です。\n",
    "ここではDatabricksエコシステムに関するサンプルテキストを使用します。\n",
    "\n",
    "実際の利用では、このディレクトリに分析したいテキストファイル(.txt)を配置してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c156f832-d992-4431-8234-1950529b24f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルデータを保存しました: /tmp/graphrag_work/input/databricks_overview.txt\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Databricksは、データエンジニアリング、データサイエンス、機械学習のための統合プラットフォームです。\n",
    "Apache Sparkの創設者たちによって設立され、レイクハウスアーキテクチャを提唱しています。\n",
    "\n",
    "Unity Catalogは、Databricksのデータガバナンスソリューションです。\n",
    "データ、MLモデル、AIアセットを一元管理し、きめ細かなアクセス制御を提供します。\n",
    "Unity CatalogはDatabricksワークスペース全体でメタデータを共有できます。\n",
    "\n",
    "MLflowは、機械学習のライフサイクル管理のためのオープンソースプラットフォームです。\n",
    "実験追跡、モデル登録、デプロイメントなどの機能を提供します。\n",
    "MLflow 3では、LoggedModelという新しい概念が導入され、GenAIアプリケーションの管理が強化されました。\n",
    "\n",
    "Delta Lakeは、データレイクに信頼性をもたらすオープンソースのストレージレイヤーです。\n",
    "ACIDトランザクション、スキーマエンフォースメント、タイムトラベルなどの機能を提供します。\n",
    "DatabricksではDelta Lakeがデフォルトのテーブル形式として使用されています。\n",
    "\n",
    "Mosaic AIは、Databricksの生成AIソリューションです。\n",
    "Foundation Modelのファインチューニング、RAGアプリケーション構築、AIエージェント開発をサポートします。\n",
    "Agent Frameworkを使用すると、本番環境向けのAIエージェントを構築できます。\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{INPUT_DIR}/databricks_overview.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"サンプルデータを保存しました: {INPUT_DIR}/databricks_overview.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e48ab83-04a1-4c63-b5c2-666d83d0f72c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. FMAPI設定\n",
    "\n",
    "Databricks Foundation Model API(FMAPI)で提供されているOpenAIモデルを使用します。\n",
    "\n",
    "**注意**: FMAPIのLlamaモデルはJSON mode制約によりGraphRAGと互換性がありません。\n",
    "OpenAIモデル(databricks-gpt-5-2等)を使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2cbdd0-98c0-4d92-b81c-6d685c1252a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks Host: https://tokyo.cloud.databricks.com\nLLM Model: databricks-gpt-5-2\nEmbedding Model: databricks-gte-large-en\n"
     ]
    }
   ],
   "source": [
    "# Databricks認証情報\n",
    "DATABRICKS_HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# FMAPI設定\n",
    "LLM_API_KEY = DATABRICKS_TOKEN\n",
    "LLM_API_BASE = f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    "\n",
    "# FMAPIで利用可能なOpenAIモデルを使用\n",
    "LLM_MODEL = \"databricks-gpt-5-2\"\n",
    "EMBED_MODEL = \"databricks-gte-large-en\"\n",
    "\n",
    "print(f\"Databricks Host: {DATABRICKS_HOST}\")\n",
    "print(f\"LLM Model: {LLM_MODEL}\")\n",
    "print(f\"Embedding Model: {EMBED_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282a0731-9757-48cb-a97f-3d90a5964437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. 設定ファイルの作成\n",
    "\n",
    "GraphRAGの動作は`settings.yaml`で制御します。\n",
    "\n",
    "**FMAPIでのポイント:**\n",
    "- `model_supports_json: true`を指定\n",
    "- `max_tokens`を明示的に指定(FMAPIは`null`を受け付けない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4591f32f-564f-483c-89cd-b506050dea42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "設定ファイルを保存しました: /tmp/graphrag_work/settings.yaml\n"
     ]
    }
   ],
   "source": [
    "settings_content = f\"\"\"encoding_model: cl100k_base\n",
    "\n",
    "models:\n",
    "  default_chat_model:\n",
    "    type: openai_chat\n",
    "    api_key: {LLM_API_KEY}\n",
    "    api_base: {LLM_API_BASE}\n",
    "    model: {LLM_MODEL}\n",
    "    encoding_model: cl100k_base\n",
    "    model_supports_json: true\n",
    "    max_tokens: 4096\n",
    "    max_retries: 3\n",
    "    request_timeout: 180\n",
    "\n",
    "  default_embedding_model:\n",
    "    type: openai_embedding\n",
    "    api_key: {LLM_API_KEY}\n",
    "    api_base: {LLM_API_BASE}\n",
    "    model: {EMBED_MODEL}\n",
    "    encoding_model: cl100k_base\n",
    "\n",
    "chunks:\n",
    "  size: 1200\n",
    "  overlap: 100\n",
    "  group_by_columns:\n",
    "    - id\n",
    "\n",
    "input:\n",
    "  type: file\n",
    "  file_type: text\n",
    "  base_dir: \"{INPUT_DIR}\"\n",
    "  file_encoding: utf-8\n",
    "  file_pattern: \".*\\\\\\\\.txt$$\"\n",
    "\n",
    "cache:\n",
    "  type: file\n",
    "  base_dir: \"{CACHE_DIR}\"\n",
    "\n",
    "storage:\n",
    "  type: file\n",
    "  base_dir: \"{OUTPUT_DIR}\"\n",
    "\n",
    "reporting:\n",
    "  type: file\n",
    "  base_dir: \"{OUTPUT_DIR}\"\n",
    "\n",
    "entity_extraction:\n",
    "  entity_types:\n",
    "    - organization\n",
    "    - person\n",
    "    - technology\n",
    "    - concept\n",
    "  max_gleanings: 1\n",
    "\n",
    "summarize_descriptions:\n",
    "  max_length: 500\n",
    "\n",
    "claim_extraction:\n",
    "  enabled: false\n",
    "\n",
    "community_reports:\n",
    "  max_length: 2000\n",
    "  max_input_length: 8000\n",
    "\n",
    "cluster_graph:\n",
    "  max_cluster_size: 10\n",
    "\n",
    "embed_graph:\n",
    "  enabled: false\n",
    "\n",
    "umap:\n",
    "  enabled: false\n",
    "\n",
    "snapshots:\n",
    "  graphml: true\n",
    "  raw_entities: true\n",
    "  top_level_nodes: true\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{WORK_DIR}/settings.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(settings_content)\n",
    "\n",
    "print(f\"設定ファイルを保存しました: {WORK_DIR}/settings.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e9726e-f33c-42d8-8bae-1c98001f3912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. インデックスの作成\n",
    "\n",
    "GraphRAGのインデックス作成プロセスでは、以下の処理が行われます:\n",
    "\n",
    "1. テキストをチャンクに分割\n",
    "2. LLMでエンティティとリレーションシップを抽出\n",
    "3. ナレッジグラフを構築\n",
    "4. コミュニティ検出(Leidenアルゴリズム)\n",
    "5. 各コミュニティのサマリーレポート生成\n",
    "6. エンティティ説明のEmbedding生成\n",
    "\n",
    "このプロセスにはLLMへの複数回のAPI呼び出しが発生するため、数分かかる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e39cf0-5776-421f-b124-ac0cd0734907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 23:08:45.568388: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2026-01-11 23:08:45.569228: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2026-01-11 23:08:45.572400: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2026-01-11 23:08:45.581585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1768172925.598047    3646 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1768172925.602666    3646 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\nW0000 00:00:1768172925.614262    3646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\nW0000 00:00:1768172925.614289    3646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\nW0000 00:00:1768172925.614296    3646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\nW0000 00:00:1768172925.614303    3646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n2026-01-11 23:08:45.617931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n/databricks/python/lib/python3.12/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\r\n  \"cipher\": algorithms.TripleDES,\r\n/databricks/python/lib/python3.12/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\r\n  \"class\": algorithms.TripleDES,\r\nModel config based on fnllm is deprecated and will be removed in GraphRAG v3, please use ModelType.Chat or ModelType.Embedding instead to switch to LiteLLM config.\r\nModel config based on fnllm is deprecated and will be removed in GraphRAG v3, please use ModelType.Chat or ModelType.Embedding instead to switch to LiteLLM config.\r\nStarting pipeline with workflows: load_input_documents, create_base_text_units, create_final_documents, extract_graph, finalize_graph, extract_covariates, create_communities, create_final_text_units, create_community_reports, generate_text_embeddings\r\nStarting workflow: load_input_documents\r\n\r\nWorkflow complete: load_input_documents\r\nStarting workflow: create_base_text_units\r\n  1 / 1 ............................................................................................\r\r\nWorkflow complete: create_base_text_units\r\nStarting workflow: create_final_documents\r\n\r\nWorkflow complete: create_final_documents\r\nStarting workflow: extract_graph\r\n  1 / 1 ............................................................................................\r  1 / 1 ............................................................................................\r  1 / 72 \r  2 / 72 \r  3 / 72 \r  4 / 72 \r  5 / 72 \r  6 / 72 \r  7 / 72 .\r  8 / 72 ..\r  9 / 72 ...\r  10 / 72 ....\r  11 / 72 .....\r  12 / 72 .......\r  13 / 72 ........\r  14 / 72 .........\r  15 / 72 ...........\r  16 / 72 ............\r  17 / 72 ..............\r  18 / 72 ...............\r  19 / 72 ................\r  20 / 72 ..................\r  21 / 72 ...................\r  22 / 72 .....................\r  23 / 72 ......................\r  24 / 72 .......................\r  25 / 72 .........................\r  26 / 72 ..........................\r  27 / 72 ............................\r  28 / 72 .............................\r  29 / 72 ..............................\r  30 / 72 ................................\r  31 / 72 .................................\r  32 / 72 ..................................\r  33 / 72 ....................................\r  34 / 72 .....................................\r  35 / 72 .......................................\r  36 / 72 ........................................\r  37 / 72 .........................................\r  38 / 72 ...........................................\r  39 / 72 ............................................\r  40 / 72 ..............................................\r  41 / 72 ...............................................\r  42 / 72 ................................................\r  43 / 72 ..................................................\r  44 / 72 ...................................................\r  45 / 72 ....................................................\r  46 / 72 ......................................................\r  47 / 72 .......................................................\r  48 / 72 .........................................................\r  49 / 72 ..........................................................\r  50 / 72 ...........................................................\r  51 / 72 .............................................................\r  52 / 72 ..............................................................\r  53 / 72 ................................................................\r  54 / 72 .................................................................\r  55 / 72 ..................................................................\r  56 / 72 ....................................................................\r  57 / 72 .....................................................................\r  58 / 72 .......................................................................\r  59 / 72 ........................................................................\r  60 / 72 .........................................................................\r  61 / 72 ...........................................................................\r  62 / 72 ............................................................................\r  63 / 72 ..............................................................................\r  64 / 72 ...............................................................................\r  65 / 72 ................................................................................\r  66 / 72 ..................................................................................\r  67 / 72 ...................................................................................\r  68 / 72 ....................................................................................\r  69 / 72 ......................................................................................\r  70 / 72 .......................................................................................\r  71 / 72 .........................................................................................\r  72 / 72 ..........................................................................................\r\r\nWorkflow complete: extract_graph\r\nStarting workflow: finalize_graph\r\n\r\nWorkflow complete: finalize_graph\r\nStarting workflow: extract_covariates\r\n\r\nWorkflow complete: extract_covariates\r\nStarting workflow: create_communities\r\n\r\nWorkflow complete: create_communities\r\nStarting workflow: create_final_text_units\r\n\r\nWorkflow complete: create_final_text_units\r\nStarting workflow: create_community_reports\r\n  1 / 1 ............................................................................................\r  1 / 5 ............\r  2 / 5 ................................\r  3 / 5 ....................................................\r  4 / 5 ........................................................................\r  5 / 5 ............................................................................................\r\r\nWorkflow complete: create_community_reports\r\nStarting workflow: generate_text_embeddings\r\n  1 / 3 .........................\r  2 / 3 ...........................................................\r  3 / 3 ............................................................................................\r\u001B[90m[\u001B[0m2026-01-11T23:10:31Z \u001B[33mWARN \u001B[0m lance::dataset::write::insert\u001B[90m]\u001B[0m No existing dataset at /tmp/graphrag_work/output/lancedb/default-entity-description.lance, it will be created\r\n  1 / 1 ............................................................................................\r\u001B[90m[\u001B[0m2026-01-11T23:10:32Z \u001B[33mWARN \u001B[0m lance::dataset::write::insert\u001B[90m]\u001B[0m No existing dataset at /tmp/graphrag_work/output/lancedb/default-community-full_content.lance, it will be created\r\n  1 / 1 ............................................................................................\r\u001B[90m[\u001B[0m2026-01-11T23:10:33Z \u001B[33mWARN \u001B[0m lance::dataset::write::insert\u001B[90m]\u001B[0m No existing dataset at /tmp/graphrag_work/output/lancedb/default-text_unit-text.lance, it will be created\r\n\r\nWorkflow complete: generate_text_embeddings\r\nPipeline complete\r\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:ThreadMonitor:Logging python thread stack frames for MainThread and py4j threads:\nDEBUG:ThreadMonitor:Logging Thread-9 (run) stack frames:\n  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n    self._bootstrap_inner()\n  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n    self.run()\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n    _threading_Thread_run(self)\n  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 521, in run\n    self.wait_for_commands()\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 593, in wait_for_commands\n    command = smart_decode(self.stream.readline())[:-1]\n  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n    return self._sock.recv_into(b)\n\nDEBUG:ThreadMonitor:Logging Thread-7 (run) stack frames:\n  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n    self._bootstrap_inner()\n  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n    self.run()\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n    _threading_Thread_run(self)\n  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 521, in run\n    self.wait_for_commands()\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 593, in wait_for_commands\n    command = smart_decode(self.stream.readline())[:-1]\n  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n    return self._sock.recv_into(b)\n\nDEBUG:ThreadMonitor:Logging Thread-4 (run) stack frames:\n  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n    self._bootstrap_inner()\n  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 2323, in run\n    readable, writable, errored = select.select(\n\nDEBUG:ThreadMonitor:Logging MainThread stack frames:\n  File \"/databricks/python_shell/scripts/db_ipykernel_launcher.py\", line 52, in <module>\n    main()\n  File \"/databricks/python_shell/scripts/db_ipykernel_launcher.py\", line 48, in main\n    DatabricksKernelApp.launch_instance(config=databricks_kernel_config())\n  File \"/databricks/python/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/databricks/python/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/databricks/python_shell/lib/dbruntime/kernel.py\", line 534, in do_execute\n    reply_content = await super().do_execute(*args, **kwargs)\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/root/.ipykernel/3562/command-6982173096383818-156383316\", line 2, in <module>\n    get_ipython().system('graphrag index --root .')\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 657, in system_piped\n    self.user_ns[\"_exit_code\"] = system(self.var_expand(cmd, depth=1))\n  File \"/databricks/python/lib/python3.12/site-packages/IPython/utils/_process_posix.py\", line 156, in system\n    res_idx = child.expect_list(patterns, self.read_timeout)\n  File \"/databricks/python/lib/python3.12/site-packages/pexpect/spawnbase.py\", line 372, in expect_list\n    return exp.expect_loop(timeout)\n  File \"/databricks/python/lib/python3.12/site-packages/pexpect/expect.py\", line 181, in expect_loop\n    return self.timeout(e)\n  File \"/databricks/python/lib/python3.12/site-packages/pexpect/pty_spawn.py\", line 510, in read_nonblocking\n    raise TIMEOUT('Timeout exceeded.')\n  File \"/databricks/python/lib/python3.12/site-packages/pexpect/pty_spawn.py\", line 450, in select\n    return select_ignore_interrupts([self.child_fd], [], [], timeout)[0]\n  File \"/databricks/python/lib/python3.12/site-packages/pexpect/utils.py\", line 143, in select_ignore_interrupts\n    return select.select(iwtd, owtd, ewtd, timeout)\n\n"
     ]
    }
   ],
   "source": [
    "os.chdir(WORK_DIR)\n",
    "!graphrag index --root ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713eff31-6715-4d1e-9768-5ff79cf1a18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. 作成されたデータの確認\n",
    "\n",
    "インデックス作成により、以下のParquetファイルが生成されます:\n",
    "\n",
    "- `entities.parquet`: 抽出されたエンティティ\n",
    "- `relationships.parquet`: エンティティ間の関係性\n",
    "- `communities.parquet`: コミュニティ構造\n",
    "- `community_reports.parquet`: 各コミュニティのサマリー\n",
    "- `text_units.parquet`: 分割されたテキストチャンク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e5a2d0-0fe2-4341-9d11-25882fef5be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 出力ファイル ===\n  communities.parquet: 10.9 KB\n  community_reports.parquet: 57.6 KB\n  context.json: 0.0 KB\n  documents.parquet: 13.2 KB\n  entities.parquet: 13.2 KB\n  graph.graphml: 4.9 KB\n  indexing-engine.log: 48.7 KB\n  relationships.parquet: 10.3 KB\n  stats.json: 1.0 KB\n  text_units.parquet: 17.1 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=== 出力ファイル ===\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    file_path = f\"{OUTPUT_DIR}/{f}\"\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path) / 1024\n",
    "        print(f\"  {f}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48ca396-1e1b-48ae-8896-94cdffa5b504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### エンティティの確認\n",
    "\n",
    "抽出されたエンティティ(人物、組織、技術、概念など)を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab4ec4d-83cf-4f12-bce6-c4d435270d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エンティティ数: 37 件\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>type</th><th>description</th></tr></thead><tbody><tr><td>DATABRICKS</td><td>ORGANIZATION</td><td>Databricks is an integrated platform for data engineering, data science, and machine learning. It was founded by the creators of Apache Spark and promotes the Lakehouse architecture. Within Databricks, products such as Unity Catalog, Delta Lake (as the default table format), and Mosaic AI are positioned as key capabilities/solutions.></td></tr><tr><td>APACHE SPARK</td><td>ORGANIZATION</td><td>Apache Spark is an open-source data processing engine whose creators founded Databricks. It is referenced as the originating technology/community behind Databricks’ founding.></td></tr><tr><td>LAKEHOUSE ARCHITECTURE</td><td>EVENT</td><td>Lakehouse architecture is an architectural approach advocated by Databricks, positioned as a unifying paradigm for data engineering/analytics and machine learning workloads on a shared data foundation.></td></tr><tr><td>UNITY CATALOG</td><td>ORGANIZATION</td><td>Unity Catalog is Databricks’ data governance solution. It centrally manages data, ML models, and AI assets, provides fine-grained access control, and enables metadata sharing across Databricks workspaces.></td></tr><tr><td>MLFLOW</td><td>ORGANIZATION</td><td>MLflow is an open-source platform for managing the machine learning lifecycle, providing capabilities such as experiment tracking, model registry, and deployment. It is described as a platform with versioned evolution including MLflow 3.></td></tr><tr><td>MLFLOW 3</td><td>EVENT</td><td>MLflow 3 is a major version release of MLflow in which a new concept called LoggedModel was introduced, strengthening management of GenAI applications.></td></tr><tr><td>LOGGEDMODEL</td><td>EVENT</td><td>LoggedModel is a new concept introduced in MLflow 3 intended to enhance management of GenAI applications within the MLflow lifecycle management framework.></td></tr><tr><td>DELTA LAKE</td><td>ORGANIZATION</td><td>Delta Lake is an open-source storage layer for data lakes that adds reliability features such as ACID transactions, schema enforcement, and time travel. In Databricks, Delta Lake is used as the default table format.></td></tr><tr><td>MOSAIC AI</td><td>ORGANIZATION</td><td>Mosaic AI is Databricks’ generative AI solution. It supports foundation model fine-tuning, building RAG applications, and developing AI agents, including production-oriented agent development via an Agent Framework.></td></tr><tr><td>AGENT FRAMEWORK</td><td>ORGANIZATION</td><td>Agent Framework is a framework referenced under Mosaic AI that enables building production-ready AI agents.></td></tr><tr><td>DATA SCIENCE</td><td>EVENT</td><td>Data science is a workload/domain supported by Databricks, involving exploratory analysis, statistical modeling, and deriving insights from data using the platform’s unified environment.></td></tr><tr><td>MACHINE LEARNING</td><td>EVENT</td><td>Machine learning is a core workload supported by Databricks and MLflow, covering model development, training, evaluation, and operationalization within a unified data/compute environment.></td></tr><tr><td>DATA GOVERNANCE</td><td>EVENT</td><td>Data governance is the discipline of managing data assets, access, policies, and metadata; in the text it is specifically addressed by Unity Catalog as Databricks’ governance solution.></td></tr><tr><td>DATASBRICKS WORKSPACES</td><td>ORGANIZATION</td><td>Databricks workspaces are the workspace environments within the Databricks platform across which Unity Catalog can share metadata and apply governance/access controls consistently.></td></tr><tr><td>METADATA</td><td>EVENT</td><td>Metadata is descriptive information about data/assets (schemas, ownership, lineage, etc.). The text states Unity Catalog can share metadata across Databricks workspaces to enable centralized governance.></td></tr><tr><td>ACCESS CONTROL</td><td>EVENT</td><td>Access control refers to mechanisms for restricting and granting permissions. Unity Catalog is described as providing fine-grained access control over data, ML models, and AI assets.></td></tr><tr><td>ML MODELS</td><td>EVENT</td><td>ML models are machine learning model artifacts that Unity Catalog can centrally manage as governed assets, and that MLflow manages through lifecycle functions such as registration and deployment.></td></tr><tr><td>AI ASSETS</td><td>EVENT</td><td>AI assets are AI-related artifacts (e.g., models, prompts, agents, applications) that Unity Catalog can centrally manage and govern alongside data and ML models.></td></tr><tr><td>EXPERIMENT TRACKING</td><td>EVENT</td><td>Experiment tracking is an MLflow capability for recording runs, parameters, metrics, and artifacts to support reproducibility and comparison of machine learning experiments.></td></tr><tr><td>MODEL REGISTRY</td><td>EVENT</td><td>Model registry is an MLflow capability for registering, versioning, and managing models for promotion through stages and deployment workflows.></td></tr><tr><td>DEPLOYMENT</td><td>EVENT</td><td>Deployment is an MLflow capability and general ML lifecycle activity for packaging and serving models/applications into target environments.></td></tr><tr><td>MACHINE LEARNING LIFECYCLE MANAGEMENT</td><td>EVENT</td><td>Machine learning lifecycle management is the end-to-end practice of managing ML work from experimentation through registration and deployment; MLflow is described as an open-source platform for this purpose.></td></tr><tr><td>OPEN SOURCE</td><td>ORGANIZATION</td><td>Open source is referenced as the development/distribution model for MLflow and Delta Lake, indicating they are community-available projects rather than proprietary-only software.></td></tr><tr><td>DATA LAKE</td><td>GEO</td><td>A data lake is the storage paradigm referenced in the text; Delta Lake is described as a storage layer that brings reliability to a data lake.></td></tr><tr><td>STORAGE LAYER</td><td>EVENT</td><td>A storage layer is the architectural component that manages how data is stored and accessed; Delta Lake is described as an open-source storage layer for data lakes.></td></tr><tr><td>ACID TRANSACTIONS</td><td>EVENT</td><td>ACID transactions are reliability guarantees (atomicity, consistency, isolation, durability) provided by Delta Lake to make data lake operations more dependable.></td></tr><tr><td>SCHEMA ENFORCEMENT</td><td>EVENT</td><td>Schema enforcement is a Delta Lake feature that ensures data written to tables conforms to expected schemas, improving data quality and reliability.></td></tr><tr><td>TIME TRAVEL</td><td>EVENT</td><td>Time travel is a Delta Lake feature enabling querying or restoring previous versions of data, supporting auditing and reproducibility.></td></tr><tr><td>TABLE FORMAT</td><td>EVENT</td><td>Table format refers to how tabular data is represented/stored; the text states Delta Lake is used as the default table format in Databricks.></td></tr><tr><td>GENERATIVE AI</td><td>EVENT</td><td>Generative AI is the AI category addressed by Mosaic AI, involving models and applications that generate text/code/other outputs and can be operationalized via RAG and agent patterns.></td></tr><tr><td>FOUNDATION MODEL</td><td>EVENT</td><td>A foundation model is a large pre-trained model; Mosaic AI supports foundation model fine-tuning as part of building GenAI solutions.></td></tr><tr><td>FINE-TUNING</td><td>EVENT</td><td>Fine-tuning is the process of adapting a pre-trained foundation model to a specific task or domain; Mosaic AI supports this capability.></td></tr><tr><td>RAG APPLICATIONS</td><td>EVENT</td><td>RAG (Retrieval-Augmented Generation) applications are GenAI applications that combine retrieval of external knowledge with generation; Mosaic AI supports building RAG applications.></td></tr><tr><td>AI AGENTS</td><td>EVENT</td><td>AI agents are autonomous or semi-autonomous systems that can plan and act; Mosaic AI (via Agent Framework) supports development of AI agents for production environments.></td></tr><tr><td>PRODUCTION ENVIRONMENT</td><td>EVENT</td><td>Production environment refers to the operational setting where systems are deployed for real users; the text notes Agent Framework enables building AI agents intended for production use.></td></tr><tr><td>GENAI APPLICATIONS</td><td>EVENT</td><td>GenAI applications are applications built using generative AI; the text states MLflow 3 strengthens management of GenAI applications via the LoggedModel concept.></td></tr><tr><td>DATA ENGINEERING</td><td></td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DATABRICKS",
         "ORGANIZATION",
         "Databricks is an integrated platform for data engineering, data science, and machine learning. It was founded by the creators of Apache Spark and promotes the Lakehouse architecture. Within Databricks, products such as Unity Catalog, Delta Lake (as the default table format), and Mosaic AI are positioned as key capabilities/solutions.>"
        ],
        [
         "APACHE SPARK",
         "ORGANIZATION",
         "Apache Spark is an open-source data processing engine whose creators founded Databricks. It is referenced as the originating technology/community behind Databricks’ founding.>"
        ],
        [
         "LAKEHOUSE ARCHITECTURE",
         "EVENT",
         "Lakehouse architecture is an architectural approach advocated by Databricks, positioned as a unifying paradigm for data engineering/analytics and machine learning workloads on a shared data foundation.>"
        ],
        [
         "UNITY CATALOG",
         "ORGANIZATION",
         "Unity Catalog is Databricks’ data governance solution. It centrally manages data, ML models, and AI assets, provides fine-grained access control, and enables metadata sharing across Databricks workspaces.>"
        ],
        [
         "MLFLOW",
         "ORGANIZATION",
         "MLflow is an open-source platform for managing the machine learning lifecycle, providing capabilities such as experiment tracking, model registry, and deployment. It is described as a platform with versioned evolution including MLflow 3.>"
        ],
        [
         "MLFLOW 3",
         "EVENT",
         "MLflow 3 is a major version release of MLflow in which a new concept called LoggedModel was introduced, strengthening management of GenAI applications.>"
        ],
        [
         "LOGGEDMODEL",
         "EVENT",
         "LoggedModel is a new concept introduced in MLflow 3 intended to enhance management of GenAI applications within the MLflow lifecycle management framework.>"
        ],
        [
         "DELTA LAKE",
         "ORGANIZATION",
         "Delta Lake is an open-source storage layer for data lakes that adds reliability features such as ACID transactions, schema enforcement, and time travel. In Databricks, Delta Lake is used as the default table format.>"
        ],
        [
         "MOSAIC AI",
         "ORGANIZATION",
         "Mosaic AI is Databricks’ generative AI solution. It supports foundation model fine-tuning, building RAG applications, and developing AI agents, including production-oriented agent development via an Agent Framework.>"
        ],
        [
         "AGENT FRAMEWORK",
         "ORGANIZATION",
         "Agent Framework is a framework referenced under Mosaic AI that enables building production-ready AI agents.>"
        ],
        [
         "DATA SCIENCE",
         "EVENT",
         "Data science is a workload/domain supported by Databricks, involving exploratory analysis, statistical modeling, and deriving insights from data using the platform’s unified environment.>"
        ],
        [
         "MACHINE LEARNING",
         "EVENT",
         "Machine learning is a core workload supported by Databricks and MLflow, covering model development, training, evaluation, and operationalization within a unified data/compute environment.>"
        ],
        [
         "DATA GOVERNANCE",
         "EVENT",
         "Data governance is the discipline of managing data assets, access, policies, and metadata; in the text it is specifically addressed by Unity Catalog as Databricks’ governance solution.>"
        ],
        [
         "DATASBRICKS WORKSPACES",
         "ORGANIZATION",
         "Databricks workspaces are the workspace environments within the Databricks platform across which Unity Catalog can share metadata and apply governance/access controls consistently.>"
        ],
        [
         "METADATA",
         "EVENT",
         "Metadata is descriptive information about data/assets (schemas, ownership, lineage, etc.). The text states Unity Catalog can share metadata across Databricks workspaces to enable centralized governance.>"
        ],
        [
         "ACCESS CONTROL",
         "EVENT",
         "Access control refers to mechanisms for restricting and granting permissions. Unity Catalog is described as providing fine-grained access control over data, ML models, and AI assets.>"
        ],
        [
         "ML MODELS",
         "EVENT",
         "ML models are machine learning model artifacts that Unity Catalog can centrally manage as governed assets, and that MLflow manages through lifecycle functions such as registration and deployment.>"
        ],
        [
         "AI ASSETS",
         "EVENT",
         "AI assets are AI-related artifacts (e.g., models, prompts, agents, applications) that Unity Catalog can centrally manage and govern alongside data and ML models.>"
        ],
        [
         "EXPERIMENT TRACKING",
         "EVENT",
         "Experiment tracking is an MLflow capability for recording runs, parameters, metrics, and artifacts to support reproducibility and comparison of machine learning experiments.>"
        ],
        [
         "MODEL REGISTRY",
         "EVENT",
         "Model registry is an MLflow capability for registering, versioning, and managing models for promotion through stages and deployment workflows.>"
        ],
        [
         "DEPLOYMENT",
         "EVENT",
         "Deployment is an MLflow capability and general ML lifecycle activity for packaging and serving models/applications into target environments.>"
        ],
        [
         "MACHINE LEARNING LIFECYCLE MANAGEMENT",
         "EVENT",
         "Machine learning lifecycle management is the end-to-end practice of managing ML work from experimentation through registration and deployment; MLflow is described as an open-source platform for this purpose.>"
        ],
        [
         "OPEN SOURCE",
         "ORGANIZATION",
         "Open source is referenced as the development/distribution model for MLflow and Delta Lake, indicating they are community-available projects rather than proprietary-only software.>"
        ],
        [
         "DATA LAKE",
         "GEO",
         "A data lake is the storage paradigm referenced in the text; Delta Lake is described as a storage layer that brings reliability to a data lake.>"
        ],
        [
         "STORAGE LAYER",
         "EVENT",
         "A storage layer is the architectural component that manages how data is stored and accessed; Delta Lake is described as an open-source storage layer for data lakes.>"
        ],
        [
         "ACID TRANSACTIONS",
         "EVENT",
         "ACID transactions are reliability guarantees (atomicity, consistency, isolation, durability) provided by Delta Lake to make data lake operations more dependable.>"
        ],
        [
         "SCHEMA ENFORCEMENT",
         "EVENT",
         "Schema enforcement is a Delta Lake feature that ensures data written to tables conforms to expected schemas, improving data quality and reliability.>"
        ],
        [
         "TIME TRAVEL",
         "EVENT",
         "Time travel is a Delta Lake feature enabling querying or restoring previous versions of data, supporting auditing and reproducibility.>"
        ],
        [
         "TABLE FORMAT",
         "EVENT",
         "Table format refers to how tabular data is represented/stored; the text states Delta Lake is used as the default table format in Databricks.>"
        ],
        [
         "GENERATIVE AI",
         "EVENT",
         "Generative AI is the AI category addressed by Mosaic AI, involving models and applications that generate text/code/other outputs and can be operationalized via RAG and agent patterns.>"
        ],
        [
         "FOUNDATION MODEL",
         "EVENT",
         "A foundation model is a large pre-trained model; Mosaic AI supports foundation model fine-tuning as part of building GenAI solutions.>"
        ],
        [
         "FINE-TUNING",
         "EVENT",
         "Fine-tuning is the process of adapting a pre-trained foundation model to a specific task or domain; Mosaic AI supports this capability.>"
        ],
        [
         "RAG APPLICATIONS",
         "EVENT",
         "RAG (Retrieval-Augmented Generation) applications are GenAI applications that combine retrieval of external knowledge with generation; Mosaic AI supports building RAG applications.>"
        ],
        [
         "AI AGENTS",
         "EVENT",
         "AI agents are autonomous or semi-autonomous systems that can plan and act; Mosaic AI (via Agent Framework) supports development of AI agents for production environments.>"
        ],
        [
         "PRODUCTION ENVIRONMENT",
         "EVENT",
         "Production environment refers to the operational setting where systems are deployed for real users; the text notes Agent Framework enables building AI agents intended for production use.>"
        ],
        [
         "GENAI APPLICATIONS",
         "EVENT",
         "GenAI applications are applications built using generative AI; the text states MLflow 3 strengthens management of GenAI applications via the LoggedModel concept.>"
        ],
        [
         "DATA ENGINEERING",
         "",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "entities_df = pd.read_parquet(f\"{OUTPUT_DIR}/entities.parquet\")\n",
    "print(f\"エンティティ数: {len(entities_df)} 件\")\n",
    "display(entities_df[[\"title\", \"type\", \"description\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28b2194-22b4-442e-a458-b92df06cc216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### リレーションシップの確認\n",
    "\n",
    "エンティティ間の関係性を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32fc2b3-9b2b-45b6-85e1-6cfa4c5ca1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "リレーションシップ数: 35 件\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>source</th><th>target</th><th>description</th></tr></thead><tbody><tr><td>DATABRICKS</td><td>APACHE SPARK</td><td>Databricks was founded by the creators of Apache Spark, linking the company/platform’s origin to the Spark project and its founders.</td></tr><tr><td>DATABRICKS</td><td>LAKEHOUSE ARCHITECTURE</td><td>Databricks advocates (promotes) the Lakehouse architecture as a core architectural approach associated with its platform.</td></tr><tr><td>DATABRICKS</td><td>UNITY CATALOG</td><td>Unity Catalog is described as Databricks’ data governance solution and operates across Databricks workspaces to share metadata and enforce access control.</td></tr><tr><td>DATABRICKS</td><td>DELTA LAKE</td><td>Delta Lake is used within Databricks as the default table format, indicating a strong product/platform integration relationship.</td></tr><tr><td>DATABRICKS</td><td>MOSAIC AI</td><td>Mosaic AI is described as Databricks’ generative AI solution, making it a first-party solution within the Databricks platform ecosystem.</td></tr><tr><td>DATABRICKS</td><td>DATA ENGINEERING</td><td>Databricks is described as an integrated platform for data engineering workloads.</td></tr><tr><td>DATABRICKS</td><td>DATA SCIENCE</td><td>Databricks is described as an integrated platform for data science workloads.</td></tr><tr><td>DATABRICKS</td><td>MACHINE LEARNING</td><td>Databricks is described as an integrated platform for machine learning workloads.</td></tr><tr><td>UNITY CATALOG</td><td>DATA GOVERNANCE</td><td>Unity Catalog is explicitly described as Databricks’ data governance solution.</td></tr><tr><td>UNITY CATALOG</td><td>ACCESS CONTROL</td><td>Unity Catalog provides fine-grained access control over governed assets.</td></tr><tr><td>UNITY CATALOG</td><td>METADATA</td><td>Unity Catalog can share metadata across Databricks workspaces.</td></tr><tr><td>UNITY CATALOG</td><td>DATASBRICKS WORKSPACES</td><td>Unity Catalog operates across Databricks workspaces to share metadata and apply governance consistently.</td></tr><tr><td>UNITY CATALOG</td><td>ML MODELS</td><td>Unity Catalog centrally manages ML models as governed assets.</td></tr><tr><td>UNITY CATALOG</td><td>AI ASSETS</td><td>Unity Catalog centrally manages AI assets as governed assets.</td></tr><tr><td>MLFLOW</td><td>MLFLOW 3</td><td>MLflow 3 is a version/release of MLflow, representing an evolution of the MLflow platform.</td></tr><tr><td>MLFLOW</td><td>MACHINE LEARNING LIFECYCLE MANAGEMENT</td><td>MLflow is described as an open-source platform for machine learning lifecycle management.</td></tr><tr><td>MLFLOW</td><td>EXPERIMENT TRACKING</td><td>MLflow provides experiment tracking functionality.</td></tr><tr><td>MLFLOW</td><td>MODEL REGISTRY</td><td>MLflow provides a model registry capability.</td></tr><tr><td>MLFLOW</td><td>DEPLOYMENT</td><td>MLflow provides deployment-related functionality for models/applications.</td></tr><tr><td>MLFLOW 3</td><td>LOGGEDMODEL</td><td>LoggedModel is explicitly introduced as a new concept in MLflow 3.</td></tr><tr><td>MLFLOW 3</td><td>GENAI APPLICATIONS</td><td>The text states MLflow 3 strengthens management of GenAI applications.</td></tr><tr><td>LOGGEDMODEL</td><td>GENAI APPLICATIONS</td><td>LoggedModel is introduced to enhance management of GenAI applications.</td></tr><tr><td>DELTA LAKE</td><td>DATA LAKE</td><td>Delta Lake is described as a storage layer that brings reliability to a data lake.</td></tr><tr><td>DELTA LAKE</td><td>STORAGE LAYER</td><td>Delta Lake is explicitly described as an open-source storage layer.</td></tr><tr><td>DELTA LAKE</td><td>ACID TRANSACTIONS</td><td>Delta Lake provides ACID transactions as a core reliability feature.</td></tr><tr><td>DELTA LAKE</td><td>SCHEMA ENFORCEMENT</td><td>Delta Lake provides schema enforcement as a core reliability feature.</td></tr><tr><td>DELTA LAKE</td><td>TIME TRAVEL</td><td>Delta Lake provides time travel as a core reliability feature.</td></tr><tr><td>DELTA LAKE</td><td>TABLE FORMAT</td><td>Delta Lake is used as the default table format in Databricks.</td></tr><tr><td>MOSAIC AI</td><td>AGENT FRAMEWORK</td><td>Agent Framework is used as part of Mosaic AI to build production-ready AI agents, indicating it is a component/capability within the Mosaic AI solution.</td></tr><tr><td>MOSAIC AI</td><td>GENERATIVE AI</td><td>Mosaic AI is described as Databricks’ generative AI solution.</td></tr><tr><td>MOSAIC AI</td><td>FOUNDATION MODEL</td><td>Mosaic AI supports foundation model fine-tuning.</td></tr><tr><td>MOSAIC AI</td><td>FINE-TUNING</td><td>Mosaic AI supports fine-tuning as a capability for adapting foundation models.</td></tr><tr><td>MOSAIC AI</td><td>RAG APPLICATIONS</td><td>Mosaic AI supports building RAG applications.</td></tr><tr><td>AGENT FRAMEWORK</td><td>AI AGENTS</td><td>Agent Framework is used to build AI agents intended for production use.</td></tr><tr><td>AGENT FRAMEWORK</td><td>PRODUCTION ENVIRONMENT</td><td>The text states Agent Framework enables building AI agents for production environments.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DATABRICKS",
         "APACHE SPARK",
         "Databricks was founded by the creators of Apache Spark, linking the company/platform’s origin to the Spark project and its founders."
        ],
        [
         "DATABRICKS",
         "LAKEHOUSE ARCHITECTURE",
         "Databricks advocates (promotes) the Lakehouse architecture as a core architectural approach associated with its platform."
        ],
        [
         "DATABRICKS",
         "UNITY CATALOG",
         "Unity Catalog is described as Databricks’ data governance solution and operates across Databricks workspaces to share metadata and enforce access control."
        ],
        [
         "DATABRICKS",
         "DELTA LAKE",
         "Delta Lake is used within Databricks as the default table format, indicating a strong product/platform integration relationship."
        ],
        [
         "DATABRICKS",
         "MOSAIC AI",
         "Mosaic AI is described as Databricks’ generative AI solution, making it a first-party solution within the Databricks platform ecosystem."
        ],
        [
         "DATABRICKS",
         "DATA ENGINEERING",
         "Databricks is described as an integrated platform for data engineering workloads."
        ],
        [
         "DATABRICKS",
         "DATA SCIENCE",
         "Databricks is described as an integrated platform for data science workloads."
        ],
        [
         "DATABRICKS",
         "MACHINE LEARNING",
         "Databricks is described as an integrated platform for machine learning workloads."
        ],
        [
         "UNITY CATALOG",
         "DATA GOVERNANCE",
         "Unity Catalog is explicitly described as Databricks’ data governance solution."
        ],
        [
         "UNITY CATALOG",
         "ACCESS CONTROL",
         "Unity Catalog provides fine-grained access control over governed assets."
        ],
        [
         "UNITY CATALOG",
         "METADATA",
         "Unity Catalog can share metadata across Databricks workspaces."
        ],
        [
         "UNITY CATALOG",
         "DATASBRICKS WORKSPACES",
         "Unity Catalog operates across Databricks workspaces to share metadata and apply governance consistently."
        ],
        [
         "UNITY CATALOG",
         "ML MODELS",
         "Unity Catalog centrally manages ML models as governed assets."
        ],
        [
         "UNITY CATALOG",
         "AI ASSETS",
         "Unity Catalog centrally manages AI assets as governed assets."
        ],
        [
         "MLFLOW",
         "MLFLOW 3",
         "MLflow 3 is a version/release of MLflow, representing an evolution of the MLflow platform."
        ],
        [
         "MLFLOW",
         "MACHINE LEARNING LIFECYCLE MANAGEMENT",
         "MLflow is described as an open-source platform for machine learning lifecycle management."
        ],
        [
         "MLFLOW",
         "EXPERIMENT TRACKING",
         "MLflow provides experiment tracking functionality."
        ],
        [
         "MLFLOW",
         "MODEL REGISTRY",
         "MLflow provides a model registry capability."
        ],
        [
         "MLFLOW",
         "DEPLOYMENT",
         "MLflow provides deployment-related functionality for models/applications."
        ],
        [
         "MLFLOW 3",
         "LOGGEDMODEL",
         "LoggedModel is explicitly introduced as a new concept in MLflow 3."
        ],
        [
         "MLFLOW 3",
         "GENAI APPLICATIONS",
         "The text states MLflow 3 strengthens management of GenAI applications."
        ],
        [
         "LOGGEDMODEL",
         "GENAI APPLICATIONS",
         "LoggedModel is introduced to enhance management of GenAI applications."
        ],
        [
         "DELTA LAKE",
         "DATA LAKE",
         "Delta Lake is described as a storage layer that brings reliability to a data lake."
        ],
        [
         "DELTA LAKE",
         "STORAGE LAYER",
         "Delta Lake is explicitly described as an open-source storage layer."
        ],
        [
         "DELTA LAKE",
         "ACID TRANSACTIONS",
         "Delta Lake provides ACID transactions as a core reliability feature."
        ],
        [
         "DELTA LAKE",
         "SCHEMA ENFORCEMENT",
         "Delta Lake provides schema enforcement as a core reliability feature."
        ],
        [
         "DELTA LAKE",
         "TIME TRAVEL",
         "Delta Lake provides time travel as a core reliability feature."
        ],
        [
         "DELTA LAKE",
         "TABLE FORMAT",
         "Delta Lake is used as the default table format in Databricks."
        ],
        [
         "MOSAIC AI",
         "AGENT FRAMEWORK",
         "Agent Framework is used as part of Mosaic AI to build production-ready AI agents, indicating it is a component/capability within the Mosaic AI solution."
        ],
        [
         "MOSAIC AI",
         "GENERATIVE AI",
         "Mosaic AI is described as Databricks’ generative AI solution."
        ],
        [
         "MOSAIC AI",
         "FOUNDATION MODEL",
         "Mosaic AI supports foundation model fine-tuning."
        ],
        [
         "MOSAIC AI",
         "FINE-TUNING",
         "Mosaic AI supports fine-tuning as a capability for adapting foundation models."
        ],
        [
         "MOSAIC AI",
         "RAG APPLICATIONS",
         "Mosaic AI supports building RAG applications."
        ],
        [
         "AGENT FRAMEWORK",
         "AI AGENTS",
         "Agent Framework is used to build AI agents intended for production use."
        ],
        [
         "AGENT FRAMEWORK",
         "PRODUCTION ENVIRONMENT",
         "The text states Agent Framework enables building AI agents for production environments."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "target",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "relationships_df = pd.read_parquet(f\"{OUTPUT_DIR}/relationships.parquet\")\n",
    "print(f\"リレーションシップ数: {len(relationships_df)} 件\")\n",
    "display(relationships_df[[\"source\", \"target\", \"description\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32068019-9b60-4515-92b3-feb1e6dd78c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### コミュニティレポートの確認\n",
    "\n",
    "関連するエンティティのクラスタ(コミュニティ)と、そのサマリーを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97cbd61f-bacc-4744-bdef-2a41e0eea158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コミュニティレポート数: 5 件\n\n=== Databricks Unity Catalog Governance: Access Control, Metadata, and Governed AI/ML Assets ===\nThis community centers on Unity Catalog, described as Databricks’ data governance solution, and its role as a hub that connects governance functions to multiple governed asset types and environments. Unity Catalog is directly linked to data governance, fine-grained access control, metadata sharing, and operation across Databricks workspaces, indicating a centralized control plane for policy and visibility. It also centrally manages ML models and broader AI assets as governed assets, suggesting the community’s focus is on consistent governance and control across both data and AI/ML artifacts within the Databricks platform. [Data: Entities (3, 12, 15, 14, 13); Relationships (8, 9, 10, 11, 12, +more)]\n\n=== Delta Lake Reliability Features for Data Lakes (ACID, Schema Enforcement, Time Travel) ===\nThis community centers on Delta Lake, an open-source storage layer positioned as a reliability-enhancing component for data lakes. The network describes Delta Lake’s functional relationships to core reliability and governance-like capabilities—ACID transactions, schema enforcement, and time travel—and also notes its role as the default table format in Databricks. Overall, the community is technical and product/architecture-focused, with Delta Lake acting as the hub entity connecting storage-layer concepts to specific reliability features and a table-format role in a platform context. [Data: Entities (7, 23, 24, 25, 26); Relationships (22, 23, 24, 25, 26, +more)]\n\n=== Agent Framework for Production AI Agents ===\nThis community centers on the Agent Framework, described as a framework referenced under Mosaic AI that enables building production-ready AI agents. The framework is directly connected to two key concepts: AI agents (the systems it is used to build) and the production environment (the operational setting those agents are intended to run in). Overall, the network depicts a straightforward capability chain: Agent Framework → AI agents → deployment in production environments, emphasizing operationalization rather than research-only experimentation. [Data: Entities (9, 33, 34); Relationships (33, 34)]\n\n=== Mosaic AI (Databricks) Generative AI Stack: Fine-Tuning, RAG, and Agent Framework ===\nThis community centers on Mosaic AI, described as Databricks’ generative AI solution, and the core capabilities it supports: foundation model fine-tuning, building RAG (Retrieval-Augmented Generation) applications, and developing AI agents via an Agent Framework. The relationships indicate a hub-and-spoke structure where Mosaic AI is the primary entity connected to the broader category of Generative AI and to specific implementation patterns and components (foundation models, fine-tuning, RAG applications, and an agent framework). Overall, the community represents an applied GenAI platform/tooling ecosystem rather than a set of individuals or a multi-organization network, with emphasis on production-oriented agent development and operational GenAI application building. [Data: Entities (8, 29, 30, 31, 32); Relationships (28, 29, 30, 31, 32)]\n\n=== Databricks, Apache Spark, and Lakehouse Architecture ===\nThis community centers on Databricks as the primary entity, positioned as an integrated platform spanning data engineering, data science, and machine learning workloads. Databricks’ origin is explicitly linked to Apache Spark through its founding by Spark’s creators, and its architectural positioning is tied to its promotion of the Lakehouse architecture as a unifying paradigm for analytics and ML on a shared data foundation. Overall, the network is a hub-and-spoke structure with Databricks as the hub and Spark, Lakehouse architecture, and the three workload domains as the spokes, indicating a cohesive product/technology ecosystem rather than a fragmented set of actors. [Data: Entities (0, 1, 2, 36, 10); Relationships (0, 1, 5, 6, 7)]\n\n"
     ]
    }
   ],
   "source": [
    "reports_df = pd.read_parquet(f\"{OUTPUT_DIR}/community_reports.parquet\")\n",
    "print(f\"コミュニティレポート数: {len(reports_df)} 件\\n\")\n",
    "\n",
    "for _, row in reports_df.iterrows():\n",
    "    print(f\"=== {row['title']} ===\")\n",
    "    print(f\"{row['summary']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcae22b2-8d47-4621-abae-fcdbd6b54d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. ナレッジグラフの可視化\n",
    "\n",
    "抽出されたエンティティとリレーションシップをグラフとして可視化します。\n",
    "PyVisを使用してインタラクティブなネットワーク図を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e16ab4e-d20c-4420-8e84-885e560dca64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "グラフ統計: 37 ノード, 35 エッジ\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "# NetworkXグラフを作成\n",
    "G = nx.Graph()\n",
    "\n",
    "# エンティティをノードとして追加\n",
    "for _, row in entities_df.iterrows():\n",
    "    G.add_node(\n",
    "        row[\"title\"],\n",
    "        title=row[\"description\"][:200] if pd.notna(row[\"description\"]) else \"\",\n",
    "        group=row[\"type\"] if pd.notna(row.get(\"type\")) else \"unknown\"\n",
    "    )\n",
    "\n",
    "# リレーションシップをエッジとして追加\n",
    "for _, row in relationships_df.iterrows():\n",
    "    if row[\"source\"] in G.nodes and row[\"target\"] in G.nodes:\n",
    "        G.add_edge(\n",
    "            row[\"source\"],\n",
    "            row[\"target\"],\n",
    "            title=row[\"description\"] if pd.notna(row[\"description\"]) else \"\"\n",
    "        )\n",
    "\n",
    "print(f\"グラフ統計: {G.number_of_nodes()} ノード, {G.number_of_edges()} エッジ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3d2e2b-5fe6-419a-8d14-516d39872f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\nグラフを保存しました: /tmp/graphrag_work/output/knowledge_graph.html\n"
     ]
    }
   ],
   "source": [
    "# PyVisでインタラクティブな可視化\n",
    "net = Network(height=\"600px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\", notebook=True)\n",
    "net.from_nx(G)\n",
    "net.toggle_physics(True)\n",
    "net.set_options(\"\"\"\n",
    "{\n",
    "  \"nodes\": {\n",
    "    \"font\": {\"size\": 14},\n",
    "    \"scaling\": {\"min\": 10, \"max\": 30}\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"color\": {\"inherit\": true},\n",
    "    \"smooth\": {\"type\": \"continuous\"}\n",
    "  },\n",
    "  \"physics\": {\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.01,\n",
    "      \"springLength\": 100\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\"\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "graph_html_path = f\"{OUTPUT_DIR}/knowledge_graph.html\"\n",
    "net.save_graph(graph_html_path)\n",
    "print(f\"グラフを保存しました: {graph_html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c69691-158b-4753-a163-50789b050c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### グラフの表示\n",
    "\n",
    "以下のセルでインタラクティブなナレッジグラフを表示します。\n",
    "ノードをドラッグして移動したり、ホバーして詳細を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be6badd-c91f-4eb6-9c1a-3164d631d611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script src=\"lib/bindings/utils.js\"></script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 100%;\n",
       "                 height: 600px;\n",
       "                 background-color: #ffffff;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"DATABRICKS\", \"label\": \"DATABRICKS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Databricks is an integrated platform for data engineering, data science, and machine learning. It was founded by the creators of Apache Spark and promotes the Lakehouse architecture. Within Databricks\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"APACHE SPARK\", \"label\": \"APACHE SPARK\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Apache Spark is an open-source data processing engine whose creators founded Databricks. It is referenced as the originating technology/community behind Databricks\\u2019 founding.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"LAKEHOUSE ARCHITECTURE\", \"label\": \"LAKEHOUSE ARCHITECTURE\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Lakehouse architecture is an architectural approach advocated by Databricks, positioned as a unifying paradigm for data engineering/analytics and machine learning workloads on a shared data foundation\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"UNITY CATALOG\", \"label\": \"UNITY CATALOG\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Unity Catalog is Databricks\\u2019 data governance solution. It centrally manages data, ML models, and AI assets, provides fine-grained access control, and enables metadata sharing across Databricks workspa\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"DELTA LAKE\", \"label\": \"DELTA LAKE\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Delta Lake is an open-source storage layer for data lakes that adds reliability features such as ACID transactions, schema enforcement, and time travel. In Databricks, Delta Lake is used as the defaul\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"MOSAIC AI\", \"label\": \"MOSAIC AI\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Mosaic AI is Databricks\\u2019 generative AI solution. It supports foundation model fine-tuning, building RAG applications, and developing AI agents, including production-oriented agent development via an A\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"\", \"id\": \"DATA ENGINEERING\", \"label\": \"DATA ENGINEERING\", \"shape\": \"dot\", \"size\": 10, \"title\": \"\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"DATA SCIENCE\", \"label\": \"DATA SCIENCE\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Data science is a workload/domain supported by Databricks, involving exploratory analysis, statistical modeling, and deriving insights from data using the platform\\u2019s unified environment.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"MACHINE LEARNING\", \"label\": \"MACHINE LEARNING\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Machine learning is a core workload supported by Databricks and MLflow, covering model development, training, evaluation, and operationalization within a unified data/compute environment.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"DATA GOVERNANCE\", \"label\": \"DATA GOVERNANCE\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Data governance is the discipline of managing data assets, access, policies, and metadata; in the text it is specifically addressed by Unity Catalog as Databricks\\u2019 governance solution.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"ACCESS CONTROL\", \"label\": \"ACCESS CONTROL\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Access control refers to mechanisms for restricting and granting permissions. Unity Catalog is described as providing fine-grained access control over data, ML models, and AI assets.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"METADATA\", \"label\": \"METADATA\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Metadata is descriptive information about data/assets (schemas, ownership, lineage, etc.). The text states Unity Catalog can share metadata across Databricks workspaces to enable centralized governanc\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"DATASBRICKS WORKSPACES\", \"label\": \"DATASBRICKS WORKSPACES\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Databricks workspaces are the workspace environments within the Databricks platform across which Unity Catalog can share metadata and apply governance/access controls consistently.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"ML MODELS\", \"label\": \"ML MODELS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"ML models are machine learning model artifacts that Unity Catalog can centrally manage as governed assets, and that MLflow manages through lifecycle functions such as registration and deployment.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"AI ASSETS\", \"label\": \"AI ASSETS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"AI assets are AI-related artifacts (e.g., models, prompts, agents, applications) that Unity Catalog can centrally manage and govern alongside data and ML models.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"MLFLOW\", \"label\": \"MLFLOW\", \"shape\": \"dot\", \"size\": 10, \"title\": \"MLflow is an open-source platform for managing the machine learning lifecycle, providing capabilities such as experiment tracking, model registry, and deployment. It is described as a platform with ve\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"MLFLOW 3\", \"label\": \"MLFLOW 3\", \"shape\": \"dot\", \"size\": 10, \"title\": \"MLflow 3 is a major version release of MLflow in which a new concept called LoggedModel was introduced, strengthening management of GenAI applications.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"MACHINE LEARNING LIFECYCLE MANAGEMENT\", \"label\": \"MACHINE LEARNING LIFECYCLE MANAGEMENT\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Machine learning lifecycle management is the end-to-end practice of managing ML work from experimentation through registration and deployment; MLflow is described as an open-source platform for this p\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"EXPERIMENT TRACKING\", \"label\": \"EXPERIMENT TRACKING\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Experiment tracking is an MLflow capability for recording runs, parameters, metrics, and artifacts to support reproducibility and comparison of machine learning experiments.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"MODEL REGISTRY\", \"label\": \"MODEL REGISTRY\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Model registry is an MLflow capability for registering, versioning, and managing models for promotion through stages and deployment workflows.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"DEPLOYMENT\", \"label\": \"DEPLOYMENT\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Deployment is an MLflow capability and general ML lifecycle activity for packaging and serving models/applications into target environments.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"LOGGEDMODEL\", \"label\": \"LOGGEDMODEL\", \"shape\": \"dot\", \"size\": 10, \"title\": \"LoggedModel is a new concept introduced in MLflow 3 intended to enhance management of GenAI applications within the MLflow lifecycle management framework.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"GENAI APPLICATIONS\", \"label\": \"GENAI APPLICATIONS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"GenAI applications are applications built using generative AI; the text states MLflow 3 strengthens management of GenAI applications via the LoggedModel concept.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"GEO\", \"id\": \"DATA LAKE\", \"label\": \"DATA LAKE\", \"shape\": \"dot\", \"size\": 10, \"title\": \"A data lake is the storage paradigm referenced in the text; Delta Lake is described as a storage layer that brings reliability to a data lake.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"STORAGE LAYER\", \"label\": \"STORAGE LAYER\", \"shape\": \"dot\", \"size\": 10, \"title\": \"A storage layer is the architectural component that manages how data is stored and accessed; Delta Lake is described as an open-source storage layer for data lakes.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"ACID TRANSACTIONS\", \"label\": \"ACID TRANSACTIONS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"ACID transactions are reliability guarantees (atomicity, consistency, isolation, durability) provided by Delta Lake to make data lake operations more dependable.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"SCHEMA ENFORCEMENT\", \"label\": \"SCHEMA ENFORCEMENT\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Schema enforcement is a Delta Lake feature that ensures data written to tables conforms to expected schemas, improving data quality and reliability.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"TIME TRAVEL\", \"label\": \"TIME TRAVEL\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Time travel is a Delta Lake feature enabling querying or restoring previous versions of data, supporting auditing and reproducibility.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"TABLE FORMAT\", \"label\": \"TABLE FORMAT\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Table format refers to how tabular data is represented/stored; the text states Delta Lake is used as the default table format in Databricks.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"AGENT FRAMEWORK\", \"label\": \"AGENT FRAMEWORK\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Agent Framework is a framework referenced under Mosaic AI that enables building production-ready AI agents.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"GENERATIVE AI\", \"label\": \"GENERATIVE AI\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Generative AI is the AI category addressed by Mosaic AI, involving models and applications that generate text/code/other outputs and can be operationalized via RAG and agent patterns.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"FOUNDATION MODEL\", \"label\": \"FOUNDATION MODEL\", \"shape\": \"dot\", \"size\": 10, \"title\": \"A foundation model is a large pre-trained model; Mosaic AI supports foundation model fine-tuning as part of building GenAI solutions.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"FINE-TUNING\", \"label\": \"FINE-TUNING\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Fine-tuning is the process of adapting a pre-trained foundation model to a specific task or domain; Mosaic AI supports this capability.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"RAG APPLICATIONS\", \"label\": \"RAG APPLICATIONS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"RAG (Retrieval-Augmented Generation) applications are GenAI applications that combine retrieval of external knowledge with generation; Mosaic AI supports building RAG applications.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"AI AGENTS\", \"label\": \"AI AGENTS\", \"shape\": \"dot\", \"size\": 10, \"title\": \"AI agents are autonomous or semi-autonomous systems that can plan and act; Mosaic AI (via Agent Framework) supports development of AI agents for production environments.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"EVENT\", \"id\": \"PRODUCTION ENVIRONMENT\", \"label\": \"PRODUCTION ENVIRONMENT\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Production environment refers to the operational setting where systems are deployed for real users; the text notes Agent Framework enables building AI agents intended for production use.\\u003e\"}, {\"font\": {\"color\": \"black\"}, \"group\": \"ORGANIZATION\", \"id\": \"OPEN SOURCE\", \"label\": \"OPEN SOURCE\", \"shape\": \"dot\", \"size\": 10, \"title\": \"Open source is referenced as the development/distribution model for MLflow and Delta Lake, indicating they are community-available projects rather than proprietary-only software.\\u003e\"}]);\n",
       "                  edges = new vis.DataSet([{\"from\": \"DATABRICKS\", \"title\": \"Databricks was founded by the creators of Apache Spark, linking the company/platform\\u2019s origin to the Spark project and its founders.\", \"to\": \"APACHE SPARK\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Databricks advocates (promotes) the Lakehouse architecture as a core architectural approach associated with its platform.\", \"to\": \"LAKEHOUSE ARCHITECTURE\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Unity Catalog is described as Databricks\\u2019 data governance solution and operates across Databricks workspaces to share metadata and enforce access control.\", \"to\": \"UNITY CATALOG\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Delta Lake is used within Databricks as the default table format, indicating a strong product/platform integration relationship.\", \"to\": \"DELTA LAKE\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Mosaic AI is described as Databricks\\u2019 generative AI solution, making it a first-party solution within the Databricks platform ecosystem.\", \"to\": \"MOSAIC AI\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Databricks is described as an integrated platform for data engineering workloads.\", \"to\": \"DATA ENGINEERING\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Databricks is described as an integrated platform for data science workloads.\", \"to\": \"DATA SCIENCE\", \"width\": 1}, {\"from\": \"DATABRICKS\", \"title\": \"Databricks is described as an integrated platform for machine learning workloads.\", \"to\": \"MACHINE LEARNING\", \"width\": 1}, {\"from\": \"UNITY CATALOG\", \"title\": \"Unity Catalog is explicitly described as Databricks\\u2019 data governance solution.\", \"to\": \"DATA GOVERNANCE\", \"width\": 1}, {\"from\": \"UNITY CATALOG\", \"title\": \"Unity Catalog provides fine-grained access control over governed assets.\", \"to\": \"ACCESS CONTROL\", \"width\": 1}, {\"from\": \"UNITY CATALOG\", \"title\": \"Unity Catalog can share metadata across Databricks workspaces.\", \"to\": \"METADATA\", \"width\": 1}, {\"from\": \"UNITY CATALOG\", \"title\": \"Unity Catalog operates across Databricks workspaces to share metadata and apply governance consistently.\", \"to\": \"DATASBRICKS WORKSPACES\", \"width\": 1}, {\"from\": \"UNITY CATALOG\", \"title\": \"Unity Catalog centrally manages ML models as governed assets.\", \"to\": \"ML MODELS\", \"width\": 1}, {\"from\": \"UNITY CATALOG\", \"title\": \"Unity Catalog centrally manages AI assets as governed assets.\", \"to\": \"AI ASSETS\", \"width\": 1}, {\"from\": \"MLFLOW\", \"title\": \"MLflow 3 is a version/release of MLflow, representing an evolution of the MLflow platform.\", \"to\": \"MLFLOW 3\", \"width\": 1}, {\"from\": \"MLFLOW\", \"title\": \"MLflow is described as an open-source platform for machine learning lifecycle management.\", \"to\": \"MACHINE LEARNING LIFECYCLE MANAGEMENT\", \"width\": 1}, {\"from\": \"MLFLOW\", \"title\": \"MLflow provides experiment tracking functionality.\", \"to\": \"EXPERIMENT TRACKING\", \"width\": 1}, {\"from\": \"MLFLOW\", \"title\": \"MLflow provides a model registry capability.\", \"to\": \"MODEL REGISTRY\", \"width\": 1}, {\"from\": \"MLFLOW\", \"title\": \"MLflow provides deployment-related functionality for models/applications.\", \"to\": \"DEPLOYMENT\", \"width\": 1}, {\"from\": \"MLFLOW 3\", \"title\": \"LoggedModel is explicitly introduced as a new concept in MLflow 3.\", \"to\": \"LOGGEDMODEL\", \"width\": 1}, {\"from\": \"MLFLOW 3\", \"title\": \"The text states MLflow 3 strengthens management of GenAI applications.\", \"to\": \"GENAI APPLICATIONS\", \"width\": 1}, {\"from\": \"LOGGEDMODEL\", \"title\": \"LoggedModel is introduced to enhance management of GenAI applications.\", \"to\": \"GENAI APPLICATIONS\", \"width\": 1}, {\"from\": \"DELTA LAKE\", \"title\": \"Delta Lake is described as a storage layer that brings reliability to a data lake.\", \"to\": \"DATA LAKE\", \"width\": 1}, {\"from\": \"DELTA LAKE\", \"title\": \"Delta Lake is explicitly described as an open-source storage layer.\", \"to\": \"STORAGE LAYER\", \"width\": 1}, {\"from\": \"DELTA LAKE\", \"title\": \"Delta Lake provides ACID transactions as a core reliability feature.\", \"to\": \"ACID TRANSACTIONS\", \"width\": 1}, {\"from\": \"DELTA LAKE\", \"title\": \"Delta Lake provides schema enforcement as a core reliability feature.\", \"to\": \"SCHEMA ENFORCEMENT\", \"width\": 1}, {\"from\": \"DELTA LAKE\", \"title\": \"Delta Lake provides time travel as a core reliability feature.\", \"to\": \"TIME TRAVEL\", \"width\": 1}, {\"from\": \"DELTA LAKE\", \"title\": \"Delta Lake is used as the default table format in Databricks.\", \"to\": \"TABLE FORMAT\", \"width\": 1}, {\"from\": \"MOSAIC AI\", \"title\": \"Agent Framework is used as part of Mosaic AI to build production-ready AI agents, indicating it is a component/capability within the Mosaic AI solution.\", \"to\": \"AGENT FRAMEWORK\", \"width\": 1}, {\"from\": \"MOSAIC AI\", \"title\": \"Mosaic AI is described as Databricks\\u2019 generative AI solution.\", \"to\": \"GENERATIVE AI\", \"width\": 1}, {\"from\": \"MOSAIC AI\", \"title\": \"Mosaic AI supports foundation model fine-tuning.\", \"to\": \"FOUNDATION MODEL\", \"width\": 1}, {\"from\": \"MOSAIC AI\", \"title\": \"Mosaic AI supports fine-tuning as a capability for adapting foundation models.\", \"to\": \"FINE-TUNING\", \"width\": 1}, {\"from\": \"MOSAIC AI\", \"title\": \"Mosaic AI supports building RAG applications.\", \"to\": \"RAG APPLICATIONS\", \"width\": 1}, {\"from\": \"AGENT FRAMEWORK\", \"title\": \"Agent Framework is used to build AI agents intended for production use.\", \"to\": \"AI AGENTS\", \"width\": 1}, {\"from\": \"AGENT FRAMEWORK\", \"title\": \"The text states Agent Framework enables building AI agents for production environments.\", \"to\": \"PRODUCTION ENVIRONMENT\", \"width\": 1}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\"nodes\": {\"font\": {\"size\": 14}, \"scaling\": {\"min\": 10, \"max\": 30}}, \"edges\": {\"color\": {\"inherit\": true}, \"smooth\": {\"type\": \"continuous\"}}, \"physics\": {\"forceAtlas2Based\": {\"gravitationalConstant\": -50, \"centralGravity\": 0.01, \"springLength\": 100}, \"solver\": \"forceAtlas2Based\"}};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(graph_html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "displayHTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d2a01a-5257-46e9-a827-ca1341810eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Global Search\n",
    "\n",
    "Global Searchは、コミュニティレポートを活用して、データセット全体に関する高レベルな質問に回答します。\n",
    "「このデータセットの主要なテーマは何か?」といった要約的な質問に適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b9f9e9-0d04-441a-b0cc-ddeabc5c6008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 23:10:47.203378: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2026-01-11 23:10:47.204210: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-01-11 23:10:47.207612: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-01-11 23:10:47.217061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768173047.233259    3562 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768173047.237923    3562 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768173047.250174    3562 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768173047.250199    3562 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768173047.250201    3562 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768173047.250203    3562 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2026-01-11 23:10:47.254461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/databricks/python/lib/python3.12/site-packages/paramiko/pkey.py:77: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n  class PKey:\n/databricks/python/lib/python3.12/site-packages/paramiko/transport.py:138: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n  class Transport(threading.Thread, ClosingContextManager):\nWARNING:graphrag.config.models.language_model_config:Model config based on fnllm is deprecated and will be removed in GraphRAG v3, please use ModelType.Chat or ModelType.Embedding instead to switch to LiteLLM config.\nWARNING:graphrag.config.models.language_model_config:Model config based on fnllm is deprecated and will be removed in GraphRAG v3, please use ModelType.Chat or ModelType.Embedding instead to switch to LiteLLM config.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import graphrag.api as api\n",
    "from graphrag.config.load_config import load_config\n",
    "\n",
    "graphrag_config = load_config(Path(WORK_DIR))\n",
    "\n",
    "entities = pd.read_parquet(f\"{OUTPUT_DIR}/entities.parquet\")\n",
    "communities = pd.read_parquet(f\"{OUTPUT_DIR}/communities.parquet\")\n",
    "community_reports = pd.read_parquet(f\"{OUTPUT_DIR}/community_reports.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d08a6c6-1407-4441-90f1-9e550563c504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 質問 ===\nこのデータセットで説明されている主要な技術とその関係性を要約してください\n\n=== 回答 ===\n## 全体像（主要技術の配置）\nこのデータセットでは、**Databricks**が統合プラットフォームとして中心に位置づけられ、**データエンジニアリング／データサイエンス／機械学習**の各ワークロードを単一基盤で扱う構造が示されています。また、Apache Sparkの創設者によって設立されたという起源的関係が示され、さらに**Lakehouseアーキテクチャ**を推進することで、分析とMLを共有データ基盤上で統合する思想が関係性として説明されています [Data: Reports (4)]。\n\n## データ基盤：Delta Lake（ストレージ層）と信頼性機能\n**Delta Lake**は、データレイク向けのオープンソース・ストレージレイヤとして中心にあり、信頼性機能として **ACIDトランザクション／スキーマ強制（schema enforcement）／タイムトラベル（過去バージョンの参照・復元）** と直接結びついています。加えて、Databricksにおける**デフォルトのテーブルフォーマット**として利用される関係が示され、プラットフォーム標準のデータ表現として位置づけられています [Data: Reports (1)]。\n\n## ガバナンス：Unity Catalog（統制のハブ）\n**Unity Catalog**はDatabricksのデータガバナンスソリューションとしてハブになっており、**データガバナンス／きめ細かなアクセス制御／メタデータ共有／複数Databricksワークスペース横断の運用**に関係づけられています。さらに、MLモデルやより広いAI資産（例：**モデル、プロンプト、エージェント、アプリケーション**）も「ガバナンス対象資産」として中央管理することで、**データとAI/ML成果物を同一の統制平面で扱う**関係性が示されています [Data: Reports (0)]。\n\n## 生成AI：Mosaic AI（生成AI機能のハブ）\n**Mosaic AI**はDatabricksの生成AIソリューションとしてハブになっており、生成AI領域の主要機能として、(1) **基盤モデル（Foundation Model）のファインチューニング**、(2) **RAG（Retrieval-Augmented Generation）アプリケーション構築**、(3) **Agent Frameworkを用いたAIエージェント開発**、という機能マップ（関係性）が示されています [Data: Reports (3)]。\n\n## エージェント：Agent Framework（本番運用への接続）\n**Agent Framework**は「本番対応のAIエージェント」を構築するための枠組みとして位置づけられ、**Agent Framework → AIエージェント → 本番環境（production environments）**という、開発から運用への直接的な関係が示されています [Data: Reports (2)]。\n\n## まとめ（関係性の要点）\n- **Databricks**が統合基盤として、データ〜ML〜生成AIまでを一つのプラットフォーム上で扱う [Data: Reports (4)]。  \n- **Delta Lake**が信頼性の高いデータ格納・テーブル表現の中核となり、分析/MLの土台を支える [Data: Reports (1)]。  \n- **Unity Catalog**がデータだけでなくAI資産まで含めて統制し、複数ワークスペース横断のガバナンスを担う [Data: Reports (0)]。  \n- **Mosaic AI**が生成AIの実装機能（ファインチューニング、RAG、エージェント）を束ね、**Agent Framework**が本番対応エージェントの構築・運用につなげる [Data: Reports (3, 2)]。\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-d245d38d4dbf1a1ea1affcdfbd8c88f7\", \"tr-dd20b269e21a194a1a45679e60e0025d\"]",
      "text/plain": [
       "[Trace(trace_id=tr-d245d38d4dbf1a1ea1affcdfbd8c88f7), Trace(trace_id=tr-dd20b269e21a194a1a45679e60e0025d)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"このデータセットで説明されている主要な技術とその関係性を要約してください\"\n",
    "\n",
    "response, context = await api.global_search(\n",
    "    config=graphrag_config,\n",
    "    entities=entities,\n",
    "    communities=communities,\n",
    "    community_reports=community_reports,\n",
    "    community_level=2,\n",
    "    dynamic_community_selection=False,\n",
    "    response_type=\"Multiple Paragraphs\",\n",
    "    query=query,\n",
    ")\n",
    "\n",
    "print(\"=== 質問 ===\")\n",
    "print(query)\n",
    "print(\"\\n=== 回答 ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14a4980-7abf-4fbd-839d-dd1e29bdd656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Local Search\n",
    "\n",
    "Local Searchは、特定のエンティティに関する詳細な質問に回答します。\n",
    "ベクトル検索でエンティティを特定し、関連するコンテキストを組み合わせて回答を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d912d7ae-231c-4352-977d-5b0b050075f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 質問 ===\nMLflowとは何ですか?どのような機能がありますか?\n\n=== 回答 ===\n## MLflowとは何か\n\nMLflowは、機械学習（ML）のライフサイクル管理のためのオープンソースプラットフォームです。具体的には、モデル開発から運用（デプロイ）までの一連の流れを管理するための仕組みを提供します [Data: Sources (0)]。\n\n## どのような機能があるか\n\n提供されている情報の範囲では、MLflowの主な機能として以下が挙げられます。\n\n- **実験追跡（Experiment Tracking）**：学習実験の結果やパラメータなどを追跡・記録する機能 [Data: Sources (0)]。  \n- **モデル登録（Model Registry）**：学習したモデルを登録し、管理する機能 [Data: Sources (0)]。  \n- **デプロイメント（Deployment）**：登録・管理したモデルを実運用環境へ展開するための機能 [Data: Sources (0)]。\n\n## MLflow 3での追加要素（提供データに基づく）\n\nMLflow 3では、**LoggedModel**という新しい概念が導入され、**生成AI（GenAI）アプリケーションの管理が強化**されたとされています [Data: Sources (0)]。\n\n## 補足（このデータで分からないこと）\n\nこのデータには、MLflowの各機能の具体的な操作方法、対応する実行環境、他のDatabricks製品（例：Unity Catalog）との詳細な連携手順までは記載がありません。そのため、ここでは上記の範囲を超える断定はできません。\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-c26dcc58c47db24794b4a463001486fc\", \"tr-e29c0716c876bc2c0fecb7cc5dc33d19\"]",
      "text/plain": [
       "[Trace(trace_id=tr-c26dcc58c47db24794b4a463001486fc), Trace(trace_id=tr-e29c0716c876bc2c0fecb7cc5dc33d19)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_units = pd.read_parquet(f\"{OUTPUT_DIR}/text_units.parquet\")\n",
    "relationships = pd.read_parquet(f\"{OUTPUT_DIR}/relationships.parquet\")\n",
    "\n",
    "query_local = \"MLflowとは何ですか?どのような機能がありますか?\"\n",
    "\n",
    "response_local, context_local = await api.local_search(\n",
    "    config=graphrag_config,\n",
    "    entities=entities,\n",
    "    communities=communities,\n",
    "    community_reports=community_reports,\n",
    "    text_units=text_units,\n",
    "    relationships=relationships,\n",
    "    covariates=None,\n",
    "    community_level=2,\n",
    "    response_type=\"Multiple Paragraphs\",\n",
    "    query=query_local,\n",
    ")\n",
    "\n",
    "print(\"=== 質問 ===\")\n",
    "print(query_local)\n",
    "print(\"\\n=== 回答 ===\")\n",
    "print(response_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60282f56-f9fc-4f33-8a77-46977d920076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Delta Tableへの保存\n",
    "\n",
    "ローカルディスク上のデータはクラスター終了時に消えてしまうため、\n",
    "Delta Tableに保存して永続化します。\n",
    "\n",
    "次回以降は、インデックス作成をスキップしてDelta Tableからデータを読み込んでクエリを実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f245c8c-1731-4148-a189-c9e64736d76f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ takaakiyayoi_catalog.default.graphrag_entities\n✓ takaakiyayoi_catalog.default.graphrag_relationships\n✓ takaakiyayoi_catalog.default.graphrag_community_reports\n✓ takaakiyayoi_catalog.default.graphrag_communities\n✓ takaakiyayoi_catalog.default.graphrag_text_units\n"
     ]
    }
   ],
   "source": [
    "tables = {\n",
    "    \"graphrag_entities\": \"entities.parquet\",\n",
    "    \"graphrag_relationships\": \"relationships.parquet\",\n",
    "    \"graphrag_community_reports\": \"community_reports.parquet\",\n",
    "    \"graphrag_communities\": \"communities.parquet\",\n",
    "    \"graphrag_text_units\": \"text_units.parquet\",\n",
    "}\n",
    "\n",
    "for table_name, file_name in tables.items():\n",
    "    file_path = f\"file:{OUTPUT_DIR}/{file_name}\"\n",
    "    spark.read.parquet(file_path).write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.{table_name}\")\n",
    "    print(f\"✓ {CATALOG}.{SCHEMA}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e90ed56-63dd-436b-aeda-f75ce8583aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. (オプション) Delta Tableからの読み込み\n",
    "\n",
    "保存したDelta Tableからデータを読み込んでクエリを実行する例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d0504e-ad27-466a-979e-f02380b99d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Delta Tableからデータを読み込む場合\n",
    "# entities = spark.read.table(f\"{CATALOG}.{SCHEMA}.graphrag_entities\").toPandas()\n",
    "# communities = spark.read.table(f\"{CATALOG}.{SCHEMA}.graphrag_communities\").toPandas()\n",
    "# community_reports = spark.read.table(f\"{CATALOG}.{SCHEMA}.graphrag_community_reports\").toPandas()\n",
    "# text_units = spark.read.table(f\"{CATALOG}.{SCHEMA}.graphrag_text_units\").toPandas()\n",
    "# relationships = spark.read.table(f\"{CATALOG}.{SCHEMA}.graphrag_relationships\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5936d3db-3d84-4c9f-9575-68ce4400fa5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、以下を実行しました:\n",
    "\n",
    "1. **インデックス作成**: テキストからエンティティ、リレーションシップ、コミュニティを抽出\n",
    "2. **可視化**: ナレッジグラフをインタラクティブに表示\n",
    "3. **Global Search**: データセット全体に関する質問に回答\n",
    "4. **Local Search**: 特定のエンティティに関する質問に回答\n",
    "5. **永続化**: 結果をDelta Tableに保存\n",
    "\n",
    "### FMAPIでの注意点\n",
    "\n",
    "| 項目 | 設定 |\n",
    "|------|------|\n",
    "| モデル | OpenAIモデル(databricks-gpt-5-2等)を使用 |\n",
    "| max_tokens | 明示的に指定(FMAPIはnullを受け付けない) |\n",
    "| model_supports_json | true |\n",
    "\n",
    "**注意**: FMAPIのLlamaモデルはJSON mode制約によりGraphRAGのクエリと互換性がありません。\n",
    "\n",
    "### その他の制約\n",
    "\n",
    "- **ストレージ**: LanceDBの制約により、クラスターのローカルディスクを使用\n",
    "- **コスト**: インデックス作成時にLLMへの多数のAPI呼び出しが発生"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "graphrag_databricks_fmapi_complete",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
